{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search\n",
    "Let's incorporate grid search into your modeling process. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ziaeeamir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ziaeeamir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ziaeeamir\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(['punkt','wordnet','averaged_perceptron_tagger'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    detected_urls = re.findall(url_regex, text)\n",
    "    for url in detected_urls:\n",
    "        text = text.replace(url, \"urlplaceholder\")\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    clean_tokens = []\n",
    "    for tok in tokens:\n",
    "        clean_tok = lemmatizer.lemmatize(tok).strip()\n",
    "        #I passed in the output from the previous noun lemmatization step. This way of chaining procedures is very common.\n",
    "        clean_tok = lemmatizer.lemmatize(clean_tok, pos='v')\n",
    "        #It is common to apply both, lemmatization first, and then stemming.\n",
    "        clean_tok =PorterStemmer().stem(clean_tok)\n",
    "        \n",
    "        clean_tokens.append(clean_tok)\n",
    "\n",
    "    return clean_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StartingVerbExtractor(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def starting_verb(self, text):\n",
    "        # tokenize by sentences\n",
    "        sentence_list = sent_tokenize(text)\n",
    "        \n",
    "        for sentence in sentence_list:\n",
    "            # tokenize each sentence into words and tag part of speech\n",
    "            pos_tags = pos_tag(tokenize(sentence))\n",
    "\n",
    "            # index pos_tags to get the first word and part of speech tag\n",
    "            first_word, first_tag = pos_tags[0]\n",
    "            \n",
    "            # return true if the first word is an appropriate verb or RT for retweet\n",
    "            if first_tag in ['VB', 'VBP'] or first_word == 'RT':\n",
    "                return True\n",
    "\n",
    "            return False\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # apply starting_verb function to all values in X\n",
    "        X_tagged = pd.Series(X).apply(self.starting_verb)\n",
    "        \n",
    "        return pd.DataFrame(X_tagged)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaseNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return pd.Series(X).apply(lambda x: x.lower()).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['implementing a custom transformer from scikit-learn'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_normalizer = CaseNormalizer()\n",
    "X = np.array(['Implementing a Custom Transformer from SCIKIT-LEARN'])\n",
    "case_normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View parameters in pipeline\n",
    "Before modifying your build_model method to include grid search, view the parameters in your pipeline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([ \n",
    "                    ('features',FeatureUnion([\n",
    "                                              ('text-pipline',Pipeline([\n",
    "                                                                        ('lowercase', CaseNormalizer()),\n",
    "                                                                        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                                                                        ('tfidf', TfidfTransformer())\n",
    "                                                                         ])),\n",
    "                                                ('starting_verb', StartingVerbExtractor())\n",
    "                            \n",
    "                                                ])),\n",
    "                         ('clf', RandomForestClassifier())\n",
    "\n",
    "                       ])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('features', FeatureUnion(n_jobs=1,\n",
       "          transformer_list=[('text-pipline', Pipeline(memory=None,\n",
       "        steps=[('lowercase', CaseNormalizer()), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_feature... smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('starting_verb', StartingVerbExtractor())],\n",
       "          transformer_weights=None)),\n",
       "  ('clf',\n",
       "   RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "               max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "               min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "               min_samples_leaf=1, min_samples_split=2,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "               oob_score=False, random_state=None, verbose=0,\n",
       "               warm_start=False))],\n",
       " 'features': FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('text-pipline', Pipeline(memory=None,\n",
       "      steps=[('lowercase', CaseNormalizer()), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_feature... smooth_idf=True, sublinear_tf=False, use_idf=True))])), ('starting_verb', StartingVerbExtractor())],\n",
       "        transformer_weights=None),\n",
       " 'clf': RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "             max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "             min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "             min_samples_leaf=1, min_samples_split=2,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "             oob_score=False, random_state=None, verbose=0,\n",
       "             warm_start=False),\n",
       " 'features__n_jobs': 1,\n",
       " 'features__transformer_list': [('text-pipline',\n",
       "   Pipeline(memory=None,\n",
       "        steps=[('lowercase', CaseNormalizer()), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=Non...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))])),\n",
       "  ('starting_verb', StartingVerbExtractor())],\n",
       " 'features__transformer_weights': None,\n",
       " 'features__text-pipline': Pipeline(memory=None,\n",
       "      steps=[('lowercase', CaseNormalizer()), ('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=Non...y=None)), ('tfidf', TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))]),\n",
       " 'features__starting_verb': StartingVerbExtractor(),\n",
       " 'features__text-pipline__memory': None,\n",
       " 'features__text-pipline__steps': [('lowercase', CaseNormalizer()),\n",
       "  ('vect',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=<function tokenize at 0x0000014B3BD23AE8>,\n",
       "           vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True))],\n",
       " 'features__text-pipline__lowercase': CaseNormalizer(),\n",
       " 'features__text-pipline__vect': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=<function tokenize at 0x0000014B3BD23AE8>,\n",
       "         vocabulary=None),\n",
       " 'features__text-pipline__tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'features__text-pipline__vect__analyzer': 'word',\n",
       " 'features__text-pipline__vect__binary': False,\n",
       " 'features__text-pipline__vect__decode_error': 'strict',\n",
       " 'features__text-pipline__vect__dtype': numpy.int64,\n",
       " 'features__text-pipline__vect__encoding': 'utf-8',\n",
       " 'features__text-pipline__vect__input': 'content',\n",
       " 'features__text-pipline__vect__lowercase': True,\n",
       " 'features__text-pipline__vect__max_df': 1.0,\n",
       " 'features__text-pipline__vect__max_features': None,\n",
       " 'features__text-pipline__vect__min_df': 1,\n",
       " 'features__text-pipline__vect__ngram_range': (1, 1),\n",
       " 'features__text-pipline__vect__preprocessor': None,\n",
       " 'features__text-pipline__vect__stop_words': None,\n",
       " 'features__text-pipline__vect__strip_accents': None,\n",
       " 'features__text-pipline__vect__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'features__text-pipline__vect__tokenizer': <function __main__.tokenize(text)>,\n",
       " 'features__text-pipline__vect__vocabulary': None,\n",
       " 'features__text-pipline__tfidf__norm': 'l2',\n",
       " 'features__text-pipline__tfidf__smooth_idf': True,\n",
       " 'features__text-pipline__tfidf__sublinear_tf': False,\n",
       " 'features__text-pipline__tfidf__use_idf': True,\n",
       " 'clf__bootstrap': True,\n",
       " 'clf__class_weight': None,\n",
       " 'clf__criterion': 'gini',\n",
       " 'clf__max_depth': None,\n",
       " 'clf__max_features': 'auto',\n",
       " 'clf__max_leaf_nodes': None,\n",
       " 'clf__min_impurity_decrease': 0.0,\n",
       " 'clf__min_impurity_split': None,\n",
       " 'clf__min_samples_leaf': 1,\n",
       " 'clf__min_samples_split': 2,\n",
       " 'clf__min_weight_fraction_leaf': 0.0,\n",
       " 'clf__n_estimators': 10,\n",
       " 'clf__n_jobs': 1,\n",
       " 'clf__oob_score': False,\n",
       " 'clf__random_state': None,\n",
       " 'clf__verbose': 0,\n",
       " 'clf__warm_start': False}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify your `build_model` function to return a GridSearchCV object.\n",
    "Try to grid search some parameters in your data transformation steps as well as those for your classifier! Browse the parameters you can search above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    pipeline = Pipeline([ \n",
    "                    ('features',FeatureUnion([\n",
    "                                              ('text-pipline',Pipeline([\n",
    "                                                                        ('lowercase', CaseNormalizer()),\n",
    "                                                                        ('vect', CountVectorizer(tokenizer=tokenize)),\n",
    "                                                                        ('tfidf', TfidfTransformer())\n",
    "                                                                         ])),\n",
    "                                                ('starting_verb', StartingVerbExtractor())\n",
    "                            \n",
    "                                                ])),\n",
    "                         ('clf', RandomForestClassifier())\n",
    "\n",
    "                       ])\n",
    "\n",
    "    # specify parameters for grid search\n",
    "    parameters = {\n",
    "                    #'features__text_pipeline__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "                    #'features__text_pipeline__vect__max_df': (0.5, 0.75, 1.0),\n",
    "                    #'features__text_pipeline__vect__max_features': (None, 5000, 10000),\n",
    "                    #'features__text_pipeline__tfidf__use_idf': (True, False),\n",
    "\n",
    "                    'clf__n_estimators': [50, 100, 200],\n",
    "                    'clf__min_samples_split': [2, 3, 4],\n",
    "                    'features__transformer_weights': (\n",
    "                                                     {'text_pipeline': 1, 'starting_verb': 0.5},\n",
    "                                                     {'text_pipeline': 0.5, 'starting_verb': 1},\n",
    "                                                     {'text_pipeline': 0.8, 'starting_verb': 1},\n",
    "                                                    )\n",
    "                 }\n",
    "\n",
    "    # create grid search object\n",
    "    cv = GridSearchCV(pipeline, parameters)\n",
    "    \n",
    "    return cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run program to test\n",
    "Running grid search can take a while, especially if you are searching over a lot of parameters! If you want to reduce it to a few minutes, try commenting out some of your parameters to grid search over just 1 or 2 parameters with a small number of values each. Once you know that works, feel free to add more parameters and see how well your final model can perform! You can try this out in the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['Action' 'Dialogue' 'Information']\n",
      "Confusion Matrix:\n",
      " [[ 81   0  28]\n",
      " [  0  24   4]\n",
      " [  5   0 459]]\n",
      "Accuracy: 0.9384359400998337\n",
      "\n",
      "Best Parameters: {'clf__min_samples_split': 2, 'clf__n_estimators': 50, 'features__transformer_weights': {'text_pipeline': 1, 'starting_verb': 0.5}}\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    df = pd.read_csv('../dataset/corporate_messaging.csv', encoding='latin-1')\n",
    "    df = df[(df[\"category:confidence\"] == 1) & (df['category'] != 'Exclude')]\n",
    "    X = df.text.values\n",
    "    y = df.category.values\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def display_results(cv, y_test, y_pred):\n",
    "    labels = np.unique(y_pred)\n",
    "    confusion_mat = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "\n",
    "    print(\"Labels:\", labels)\n",
    "    print(\"Confusion Matrix:\\n\", confusion_mat)\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"\\nBest Parameters:\", cv.best_params_)\n",
    "\n",
    "\n",
    "def main():\n",
    "    X, y = load_data()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    display_results(model, y_test, y_pred)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
